# -*- coding: utf-8 -*-
"""TextAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZvPJwRhQ23ZfK6B6om0vpmmJqvLRNTa9
"""

from bs4 import BeautifulSoup
import urllib.request
from IPython.display import HTML
import requests
import nltk
from nltk import tokenize
from nltk.tokenize import word_tokenize
nltk.download('punkt')
import re

"""**Data Extraction And Data Preprocessing**"""

url = 'https://insights.blackcoffer.com/challenges-and-opportunities-of-big-data-in-healthcare/'

page = requests.get(url, headers={"User-Agent": "XY"})

page.content

BeautifulSoup(page.content, 'html.parser')

soup = BeautifulSoup(page.content, 'html.parser')

title = soup.find('h1', class_ ='entry-title') ##extracting title
print(title.text)

title = title.text

title

title = [title]

article = [] ##extracting article
for para in soup.find_all("p"):
  x = para.get_text()
  article.append(x)

article

data = title + article

data

data = [x for x in data if x]  
data

data2 = ' '.join(data)
data2

##with open("150.txt", "w") as output:
##    output.write(str(data2))                        #to save the extracted text data (list) into a txt file.

data2_lower = data2.lower()

data2_lower

data2_lower_tokenized_word = tokenize.word_tokenize(data2_lower) ##tokenizing data by word 
data2_lower_tokenized_word

data2_lower_tokenized_sentence = tokenize.sent_tokenize(data2_lower) ##tokenizing data by sentence
data2_lower_tokenized_sentence

data2_lower_tokenized_word_no_p = [word for word in data2_lower_tokenized_word if word.isalpha()] ##removing puntucations
print(data2_lower_tokenized_word_no_p)

stopwordstxt = open("stopwords.txt", "r")
stopword = stopwordstxt.read()
stopwords = stopword.lower()
stopwords = stopwords.split('\n')
stopwordstxt.close()
stopwords

data2_lower_tokenized_word_no_p_no_sw = [w for w in data2_lower_tokenized_word_no_p if not w in stopwords] ##removing stopwords
print(data2_lower_tokenized_word_no_p_no_sw)

"""**Import positive-words.text and negative-words.txt**"""

positivewordstxt = open("positive-words.txt", "r")
positiveword = positivewordstxt.read()
positivewords = positiveword.lower()
positivewords = positivewords.split('\n')
positivewordstxt.close()
positivewords

negativewordstxt = open("negative-words.txt","r", encoding="ISO-8859-1")
negativeword = negativewordstxt.read()
negativewords = negativeword.lower()
negativewords = negativewords.split('\n')
negativewordstxt.close()
negativewords

"""***Text Analysis***

**Positive Score**
"""

def positive_score(text):
    numPosWords = 0
    for word in text:
        if word in positivewords:
            numPosWords  += 1
    
    sumPos = numPosWords
    return sumPos

PositiveScore = positive_score(data2_lower_tokenized_word_no_p_no_sw)
PositiveScore

"""**Negative Score**"""

def negative_score(text):
    numNegWords=0
    for word in text:
        if word in negativewords:
            numNegWords -=1
    sumNeg = numNegWords 
    sumNeg = sumNeg * -1
    return sumNeg

NegativeScore = negative_score(data2_lower_tokenized_word_no_p_no_sw)
NegativeScore

"""**Polarity Score**"""

def polarity_score(positiveScore, negativeScore):
    pol_score = (positiveScore - negativeScore) / ((positiveScore + negativeScore) + 0.000001)
    return pol_score

PolarityScore = polarity_score(PositiveScore,NegativeScore)
PolarityScore

"""**Subjective Score**"""

def subjectivity_score(positiveScore, negativeScore):
    sub_score = (positiveScore + negativeScore) / (len(data2_lower_tokenized_word_no_p_no_sw) + 0.000001)
    return sub_score

SubjectiveScore = subjectivity_score(PositiveScore,NegativeScore)
SubjectiveScore

"""**Average Sentence Length**"""

def average_sentence_length(text, text2):
   
    totalWordCount = len(text)
    totalSentences = len(text2)
    average_sent = 0
    if totalSentences != 0:
        average_sent = totalWordCount / totalSentences
    
    average_sent_length= average_sent
    
    return (average_sent_length)

AverageSentenceLength = average_sentence_length(data2_lower_tokenized_word_no_p_no_sw,data2_lower_tokenized_sentence)
AverageSentenceLength

"""**Percentage Complex Word**"""

def percentage_complex_word(text):
    complexWord = 0
    complex_word_percentage = 0
    
    for word in text:
        vowels=0
        if word.endswith(('es','ed')):
            pass
        else:
            for w in word:
                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):
                    vowels += 1
            if(vowels > 2):
                complexWord += 1
    if len(text) != 0:
        complex_word_percentage = complexWord/len(text)
    
    return complex_word_percentage

PercentageComplexWord = percentage_complex_word(data2_lower_tokenized_word_no_p_no_sw)
PercentageComplexWord

"""**Fog Index**"""

def fog_index(averageSentenceLength, percentageComplexWord):
    fogIndex = 0.4 * (averageSentenceLength + percentageComplexWord)
    return fogIndex

FogIndex = fog_index(AverageSentenceLength,PercentageComplexWord)
FogIndex

"""**Word Count**"""

WordCount = len(data2_lower_tokenized_word_no_p_no_sw)
WordCount

"""**Complex Word Count**"""

def complex_word_count(text):
    complexWord = 0
    
    for word in text:
        vowels=0
        if word.endswith(('es','ed')):
            pass
        else:
            for w in word:
                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):
                    vowels += 1
            if(vowels > 2):
                complexWord += 1
    return complexWord

ComplexWordCount = complex_word_count(data2_lower_tokenized_word_no_p_no_sw)
ComplexWordCount

"""**Average Number Of Words Per Sentence**"""

AverageNumberOfWordsPerSentence = len(data2_lower_tokenized_word_no_p)/len(data2_lower_tokenized_sentence) ##here I didn't used the dataset with stopwords removed as in the Text Analyis.docx file, "total number of words"/total number of sentences is mentioned, but we can simply substitute the variable containing the dataset with stopwords removed by just replace the variable name to data2_lower_tokenized_word_no_p_no_sw. 
AverageNumberOfWordsPerSentence

"""**Syllable Per Word And Average Number Of Syllable Per Word**"""

!pip install syllapy
import syllapy

SyllablePerWord = []
for i  in data2_lower_tokenized_word_no_p_no_sw:
  o = syllapy.count(i)
  SyllablePerWord.append(o)

SyllablePerWord

print(*SyllablePerWord, sep = ", ") ##This returns list of number of syllables per word

AverageSyllablePerWord = sum(SyllablePerWord)/len(SyllablePerWord) ##this returns average number of syllable per word, I was not sure if i need to submit an average number of syllables per word or syllables per word. I submitted syllables per word as it was written in the Text Analysis.docx.
AverageSyllablePerWord

"""**Personal Pronouns**"""

pronounRegex = re.compile(r'\b(?-i:I)\b|\bwe\b|\bWe\b|\bmy\b|\bMy\b|\bours\b|\b(?-i:us)\b|\bOurs\b',re.I)
pronouns = pronounRegex.findall(data2)
PronounCount = len(pronouns)
print(pronouns)
PronounCount

"""**Average Word Length**"""

def average_word_length(text):
  no_of_words = len(text)
  p = ''.join(text)
  no_of_characters = len(p)

  avg_word_len = no_of_characters/no_of_words

  return avg_word_len

AverageWordLength = average_word_length(data2_lower_tokenized_word_no_p)
AverageWordLength